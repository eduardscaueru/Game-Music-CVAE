1. Modelul face overfit pe blade runner cu F1 de 0.93 dupa vreo 500 epoci cu urmatoarea configuratie:
ENCODER_UNITS = 128
ENCODER_UNITS_2 = 64
# ENCODER_UNITS_3 = 512
LATENT_DIM = 32
BARS = 0.5
BATCH_SIZE = 4
FS = 16
NUM_INSTRUMENTS = 1
MAX_INSTRUMENTS_PER_SONG = 2
+ Layer normalization

2. F1 de >93 dupa 2000 epoci pe tot dataset ul fara horror si fara indiana jones cu urmatoarea configuratie:
ENCODER_UNITS = 350
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 256
LATENT_DIM = 128
BETA = 0.01
EPOCHS = 15001
BARS = 0.5
BATCH_SIZE = 32
FS = 8
NUM_INSTRUMENTS = 8
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

3. F1 de >99 dupa 1600 epoci pe tot dataset ul fara horror si fara indiana jones cu urmatoarea configuratie:
ENCODER_UNITS = 350
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 256
LATENT_DIM = 128
BETA = 0.01
EPOCHS = 2501
BARS = 0.5
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 8
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

4. F1 de >96 dupa 1000 epoci pe tot dataset ul fara indiana jones cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 256
LATENT_DIM = 128
BETA = 0.01
EPOCHS = 2501
BARS = 0.5
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 10
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

5. F1 de >96 dupa 1800 epoci pe tot dataset ul fara indiana jones cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 256
LATENT_DIM = 128
BETA = 0.01
EPOCHS = 2501
BARS = 1
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 10
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

6. F1 de >... dupa ... epoci pe tot dataset ul fara indiana jones cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 256
LATENT_DIM = 128
BETA = 5.25 (tot nu converge dar cica trebuie ca BETA sa fie N/M (https://qr.ae/pyFFxM) (0.1 converge greu)
Daca beta nu exista atunci modelul cica va invata o melodie/output pentru toate exemplele din spatiul latent. Dar aici (https://stackoverflow.com/questions/71692032/vae-reconstruction-loss-mse-not-decreasing-but-kl-divergence-is) zice ca BETA e 1 si poate fi si mai mic dar atunci nu mai poti avea disentangled latent space.
EPOCHS = 6001
BARS = 1
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 10
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

7. F1 de >80 dupa 1700 epoci, dar dupa a inceput sa scada pana la 40, iar apoi sa urce pana la 65 la sfarsit pe tot dataset ul (kung_fu in loc de doom) fara indiana jones cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 128
LATENT_DIM = 64
BETA = 0.01
EPOCHS = 2001
BARS = 1
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 10
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
optimizer = keras.optimizers.Adam()

8. F1 de >96 dupa 1600 epoci pe tot dataset ul (kung_fu in loc de doom) fara indiana jones cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 128
LATENT_DIM = 64
BETA = 0.01
EPOCHS = 2001
BARS = 1
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 10
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

9. F1 de >... dupa ... epoci pe tot dataset ul (kung_fu in loc de doom) fara indiana jones cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 128
LATENT_DIM = 64
BETA = frange_cycle_sigmoid
EPOCHS = 2001
BARS = 1
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 10
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

10. F1 de >... dupa ... epoci pe tot dataset ul (kung_fu in loc de doom) fara indiana jones cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 128
LATENT_DIM = 64
BETA = frange_cycle_cosine
EPOCHS = 4001
BARS = 1
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 10
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

11. F1 de >96 dupa 2500 epoci pe tot dataset ul (kung_fu in loc de doom) fara indiana jones cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 128
LATENT_DIM = 64
BETA = frange_cycle_cosine
EPOCHS = 4001
BARS = 1
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 14
MAX_INSTRUMENTS_PER_SONG = 2
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)


12. F1 de >45 dupa 3000 epoci pe tot dataset ul fara indiana jones cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 256
LATENT_DIM = 128
BETA = 0.01
EPOCHS = 3001
BARS = 2
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 10
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

13. F1 de >94 dupa 2400 epoci pe tot dataset ul nou cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 128
LATENT_DIM = 64
BETA = 0.01
EPOCHS = 3001
BARS = 1
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 13
MAX_INSTRUMENTS_PER_SONG = 2
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

14. F1 de >.. dupa ... epoci pe tot dataset ul nou cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 128
LATENT_DIM = 64
BETA = frange_cycle_sigmoid
EPOCHS = 4001
BARS = 1
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 13
MAX_INSTRUMENTS_PER_SONG = 2
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

15. F1 de >25 dupa 3001 epoci pe tot dataset ul nou cu urmatoarea configuratie:
ENCODER_UNITS = 512
ENCODER_UNITS_2 = 256
ENCODER_UNITS_3 = 128
LATENT_DIM = 64
BETA = 5.25 (1.86 (beta_norm) tot la fel)
EPOCHS = 3001
BARS = 1
BATCH_SIZE = 32
FS = 16
NUM_INSTRUMENTS = 13
MAX_INSTRUMENTS_PER_SONG = 2
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)

17. F1 de >75 (de rulat cu mai multe epoci) dupa 4000 epoci pe urmatorul dataset augumentat cu urmatoarea configuratie:
[256, 128, 64]
LATENT_DIM = 32
BETA = 1
EPOCHS = 4000
BARS = 1
BATCH_SIZE = 64
FS = 16
NUM_INSTRUMENTS = 5
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)
metroid + burning_moneky genereaza bass nou spre exemplu
styles = [
    [
        'data/action/kung_fu',
        'data/action/metroid2',
        'data/action/r-type'
    ],
    [
        'data/adventure/blade_runner',
        'data/adventure/castlevania2'
        # 'data/adventure/myst'
    ],
    [
        'data/arcade/blox',
        'data/arcade/burning_monkey'
        # 'data/arcade/mario'
    ]
    # [
    #     'data/horror/blood',
    #     'data/horror/house_of_the_dead'
    # ]
]

18. F1 de >96 dupa 4000 epoci pe urmatorul dataset augumentat cu urmatoarea configuratie:
[256, 128, 64]
LATENT_DIM = 32
BETA = cosine
EPOCHS = 4000
BARS = 1
BATCH_SIZE = 64
FS = 16
NUM_INSTRUMENTS = 5
MAX_INSTRUMENTS_PER_SONG = 1
MAX_INSTRUMENTS_GENERATED = 3
lr = 1.0
optimizer = keras.optimizers.Adadelta(learning_rate=lr)
metroid + burning_moneky genereaza bass nou spre exemplu
styles = [
    [
        'data/action/kung_fu',
        'data/action/metroid2',
        'data/action/r-type'
    ],
    [
        'data/adventure/blade_runner',
        'data/adventure/castlevania2'
        # 'data/adventure/myst'
    ],
    [
        'data/arcade/blox',
        'data/arcade/burning_monkey'
        # 'data/arcade/mario'
    ]
    # [
    #     'data/horror/blood',
    #     'data/horror/house_of_the_dead'
    # ]
]
